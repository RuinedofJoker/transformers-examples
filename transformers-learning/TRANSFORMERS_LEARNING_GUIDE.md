# Hugging Face Transformers å­¦ä¹ æŒ‡å—

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬å­¦ä¹ æŒ‡å—åŸºäº Hugging Face Transformers å®˜æ–¹ä¸­æ–‡æ–‡æ¡£æ•´ç†ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿå­¦ä¹ å’ŒæŒæ¡ Transformers åº“çš„ä½¿ç”¨ã€‚

**æ–‡æ¡£æ¥æº**: https://hugging-face.cn/docs/transformers/

**æœ€åæ›´æ–°**: 2025-02-07

---

## ğŸ¯ ä»€ä¹ˆæ˜¯ Transformersï¼Ÿ

**Transformers** æ˜¯ Hugging Face æä¾›çš„æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¨¡å‹å®šä¹‰æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æ ¸å¿ƒç‰¹æ€§

1. **å¤šæ¨¡æ€æ”¯æŒ**
   - æ–‡æœ¬å¤„ç†
   - è®¡ç®—æœºè§†è§‰
   - éŸ³é¢‘å¤„ç†
   - è§†é¢‘å¤„ç†
   - å¤šæ¨¡æ€æ¨¡å‹

2. **è·¨æ¡†æ¶å…¼å®¹**
   - è®­ç»ƒæ¡†æ¶ï¼šAxolotlã€Unslothã€DeepSpeedã€FSDPã€PyTorch-Lightning
   - æ¨ç†å¼•æ“ï¼švLLMã€SGLangã€TGI
   - å»ºæ¨¡åº“ï¼šllama.cppã€mlx

3. **æµ·é‡æ¨¡å‹èµ„æº**
   - Hugging Face Hub ä¸Šæœ‰è¶…è¿‡ 100 ä¸‡ä¸ªæ¨¡å‹æ£€æŸ¥ç‚¹
   - æ”¯æŒæœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹
   - æ¨¡å‹å®šä¹‰ç®€å•ã€å¯å®šåˆ¶ä¸”é«˜æ•ˆ

---

## ğŸ—ï¸ è®¾è®¡ç†å¿µ

### ä¸‰ä¸ªæ ¸å¿ƒæŠ½è±¡

Transformers çš„è®¾è®¡æå…¶ç®€æ´ï¼Œåªæœ‰ä¸‰ä¸ªæ ¸å¿ƒç±»ï¼š

| ç±»åˆ« | è¯´æ˜ |
|------|------|
| **PretrainedConfig** | é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šæ¨¡å‹å±æ€§ï¼ˆå¦‚æ³¨æ„åŠ›å¤´æ•°ã€è¯æ±‡è¡¨å¤§å°ï¼‰ |
| **PreTrainedModel** | æ¨¡å‹æ¶æ„ï¼Œç”±é…ç½®æ–‡ä»¶å®šä¹‰ï¼Œè¿”å›åŸå§‹éšè—çŠ¶æ€ |
| **Preprocessor** | é¢„å¤„ç†å™¨ï¼Œå°†åŸå§‹è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹æ•°å€¼è¾“å…¥ |

### ä¸¤ä¸ªæ ¸å¿ƒ API

1. **Pipeline** - ç”¨äºæ¨ç†
2. **Trainer** - ç”¨äºè®­ç»ƒ

### è®¾è®¡åŸåˆ™

- **å¿«é€Ÿæ˜“ç”¨**ï¼šæ¯ä¸ªæ¨¡å‹ä»…ç”±ä¸‰ä¸ªä¸»è¦ç±»å®ç°
- **é¢„è®­ç»ƒä¼˜å…ˆ**ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å‡å°‘ç¢³è¶³è¿¹ã€è®¡ç®—æˆæœ¬å’Œæ—¶é—´
- **æœ€å…ˆè¿›æ€§èƒ½**ï¼šæ¯ä¸ªé¢„è®­ç»ƒæ¨¡å‹éƒ½å°½å¯èƒ½å¤ç°åŸå§‹æ¨¡å‹æ€§èƒ½

---

## ğŸ“š å­¦ä¹ è·¯çº¿

### ç¬¬ä¸€é˜¶æ®µï¼šå…¥é—¨åŸºç¡€ï¼ˆ1-2å‘¨ï¼‰

#### 1. ç¯å¢ƒè®¾ç½®
- åˆ›å»º Hugging Face è´¦æˆ·
- å®‰è£…å¿…è¦çš„åº“
- é…ç½®å¼€å‘ç¯å¢ƒ

#### 2. å¿«é€Ÿå…¥é—¨
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
- ä½¿ç”¨ Pipeline è¿›è¡Œæ¨ç†
- ä½¿ç”¨ Trainer å¾®è°ƒæ¨¡å‹

#### 3. æ ¸å¿ƒæ¦‚å¿µ
- ç†è§£ä¸‰ä¸ªåŸºç±»çš„ä½œç”¨
- æŒæ¡ AutoClass API
- å­¦ä¹ æ¨¡å‹åŠ è½½å’Œä¿å­˜

### ç¬¬äºŒé˜¶æ®µï¼šæ·±å…¥å­¦ä¹ ï¼ˆ2-3å‘¨ï¼‰

#### 1. Pipeline æ·±å…¥
- æ–‡æœ¬ç”Ÿæˆ
- å›¾åƒåˆ†å‰²
- è‡ªåŠ¨è¯­éŸ³è¯†åˆ«
- æ–‡æ¡£é—®ç­”
- å…¶ä»–ä»»åŠ¡

#### 2. Trainer æ·±å…¥
- è®­ç»ƒå‚æ•°é…ç½®
- åˆ†å¸ƒå¼è®­ç»ƒ
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ€§èƒ½ä¼˜åŒ–

#### 3. æ¨¡å‹å®šåˆ¶
- è‡ªå®šä¹‰æ¨¡å‹æ¶æ„
- é¢„å¤„ç†å™¨å®šåˆ¶
- é…ç½®æ–‡ä»¶ç®¡ç†

### ç¬¬ä¸‰é˜¶æ®µï¼šé«˜çº§åº”ç”¨ï¼ˆ3-4å‘¨ï¼‰

#### 1. é‡åŒ–æŠ€æœ¯
- å‡å°‘å†…å­˜å ç”¨
- åŠ é€Ÿæ¨ç†
- æƒé‡å‹ç¼©

#### 2. ç”Ÿäº§éƒ¨ç½²
- æ¨¡å‹å¯¼å‡º
- æ¨ç†ä¼˜åŒ–
- æœåŠ¡åŒ–éƒ¨ç½²

#### 3. ç‰¹å®šä»»åŠ¡
- LLM åº”ç”¨
- è§†è§‰è¯­è¨€æ¨¡å‹
- å¤šæ¨¡æ€ä»»åŠ¡

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

#### 1. åˆ›å»º Hugging Face è´¦æˆ·

è®¿é—® https://hugging-face.cn/join åˆ›å»ºè´¦æˆ·ï¼Œè¿™æ ·å¯ä»¥ï¼š
- æ‰˜ç®¡å’Œè®¿é—®æ¨¡å‹
- ä½¿ç”¨æ•°æ®é›†
- åˆ›å»º Spacesï¼ˆåº”ç”¨ç©ºé—´ï¼‰

#### 2. å®‰è£…ä¾èµ–

**å®‰è£… PyTorch**:
```bash
pip install torch
```

**å®‰è£… Transformers åŠç”Ÿæ€åº“**:
```bash
pip install -U transformers datasets evaluate accelerate timm
```

**åº“è¯´æ˜**:
- `transformers`: æ ¸å¿ƒåº“+
- `datasets`: æ•°æ®é›†åŠ è½½å’Œå¤„ç†
- `evaluate`: æ¨¡å‹è¯„ä¼°
- `accelerate`: åˆ†å¸ƒå¼è®­ç»ƒå’Œä¼˜åŒ–
- `timm`: è§†è§‰æ¨¡å‹æ”¯æŒ

#### 3. ç™»å½•è´¦æˆ·

**åœ¨ Notebook ä¸­**:
```python
from huggingface_hub import notebook_login
notebook_login()
```

**åœ¨ CLI ä¸­**:
```bash
huggingface-cli login
```

---

## ğŸ’¡ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. Pipeline - æœ€ç®€å•çš„æ¨ç†æ–¹å¼

Pipeline æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†æœ€ä¾¿æ·çš„æ–¹å¼ã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from transformers import pipeline

# åˆ›å»ºæ–‡æœ¬ç”Ÿæˆ pipeline
generator = pipeline("text-generation", model="meta-llama/Llama-2-7b-hf", device="cuda")

# ç”Ÿæˆæ–‡æœ¬
result = generator("The secret to baking a good cake is ", max_length=50)
print(result)
```

#### æ”¯æŒçš„ä»»åŠ¡

- **æ–‡æœ¬ä»»åŠ¡**: æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘
- **è§†è§‰ä»»åŠ¡**: å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²
- **éŸ³é¢‘ä»»åŠ¡**: è¯­éŸ³è¯†åˆ«ã€éŸ³é¢‘åˆ†ç±»
- **å¤šæ¨¡æ€**: è§†è§‰é—®ç­”ã€å›¾åƒæè¿°

#### æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨ GPU åŠ é€Ÿ
pipeline = pipeline("text-generation", model="model-name", device="cuda")

# æ‰¹é‡å¤„ç†
results = pipeline(["text1", "text2", "text3"])
```

### 2. é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

#### ä½¿ç”¨ AutoClass API

æ¨èä½¿ç”¨ `AutoClass` APIï¼Œå®ƒä¼šè‡ªåŠ¨æ¨æ–­é€‚åˆçš„æ¨¡å‹æ¶æ„ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype="auto",      # è‡ªåŠ¨é€‰æ‹©æ•°æ®ç±»å‹
    device_map="auto"        # è‡ªåŠ¨åˆ†é…åˆ°æœ€å¿«è®¾å¤‡
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
```

#### æ¨¡å‹æ¨ç†

```python
# å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
model_inputs = tokenizer(["The secret to baking a good cake is "], return_tensors="pt").to("cuda")

# ç”Ÿæˆæ–‡æœ¬
generated_ids = model.generate(**model_inputs, max_length=30)

# è§£ç å›æ–‡æœ¬
result = tokenizer.batch_decode(generated_ids)[0]
print(result)
```

#### é‡è¦å‚æ•°è¯´æ˜

- `device_map="auto"`: è‡ªåŠ¨å°†æ¨¡å‹æƒé‡åˆ†é…åˆ°æœ€å¿«çš„è®¾å¤‡ï¼ˆé€šå¸¸æ˜¯ GPUï¼‰
- `torch_dtype="auto"`: ç›´æ¥ä»¥æƒé‡å­˜å‚¨çš„æ•°æ®ç±»å‹åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åŠ è½½

### 3. Trainer - å®Œæ•´çš„è®­ç»ƒå¾ªç¯

Trainer æ˜¯ PyTorch æ¨¡å‹çš„å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ï¼ŒæŠ½è±¡äº†å¤§é‡æ ·æ¿ä»£ç ã€‚

#### åŸºæœ¬è®­ç»ƒæµç¨‹

**æ­¥éª¤ 1: åŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨å’Œæ•°æ®é›†**

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("rotten_tomatoes")
```

**æ­¥éª¤ 2: æ•°æ®é¢„å¤„ç†**

```python
# å®šä¹‰åˆ†è¯å‡½æ•°
def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])

# åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†
dataset = dataset.map(tokenize_dataset, batched=True)
```

**æ­¥éª¤ 3: é…ç½®æ•°æ®æ•´ç†å™¨**

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

**æ­¥éª¤ 4: é…ç½®è®­ç»ƒå‚æ•°**

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="distilbert-rotten-tomatoes",  # è¾“å‡ºç›®å½•
    learning_rate=2e-5,                       # å­¦ä¹ ç‡
    per_device_train_batch_size=8,            # è®­ç»ƒæ‰¹é‡å¤§å°
    per_device_eval_batch_size=8,             # è¯„ä¼°æ‰¹é‡å¤§å°
    num_train_epochs=2,                       # è®­ç»ƒè½®æ•°
    push_to_hub=True,                         # æ˜¯å¦æ¨é€åˆ° Hub
)
```

**æ­¥éª¤ 5: åˆ›å»º Trainer å¹¶å¼€å§‹è®­ç»ƒ**

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# æ¨é€åˆ° Hub
trainer.push_to_hub()
```

#### TrainingArguments å¸¸ç”¨å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `learning_rate` | å­¦ä¹ ç‡ | 2e-5 ~ 5e-5 |
| `per_device_train_batch_size` | æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹é‡å¤§å° | 8 ~ 32 |
| `num_train_epochs` | è®­ç»ƒè½®æ•° | 2 ~ 5 |
| `warmup_steps` | é¢„çƒ­æ­¥æ•° | 500 |
| `weight_decay` | æƒé‡è¡°å‡ | 0.01 |
| `logging_dir` | æ—¥å¿—ç›®å½• | "./logs" |
| `save_strategy` | ä¿å­˜ç­–ç•¥ | "epoch" |
| `evaluation_strategy` | è¯„ä¼°ç­–ç•¥ | "epoch" |

---

## ğŸ“– å­¦ä¹ å»ºè®®

### æ¨èå­¦ä¹ é¡ºåº

1. **ç¬¬1å‘¨ï¼šåŸºç¡€å…¥é—¨**
   - å®Œæˆç¯å¢ƒé…ç½®
   - å­¦ä¹  Pipeline åŸºæœ¬ç”¨æ³•
   - å°è¯•ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹

2. **ç¬¬2å‘¨ï¼šæ¨¡å‹åŠ è½½ä¸æ¨ç†**
   - æŒæ¡ AutoClass API
   - ç†è§£æ¨¡å‹åŠ è½½å‚æ•°
   - å®è·µæ–‡æœ¬ç”Ÿæˆä»»åŠ¡

3. **ç¬¬3å‘¨ï¼šæ¨¡å‹å¾®è°ƒ**
   - å­¦ä¹  Trainer ä½¿ç”¨
   - å®Œæˆæƒ…æ„Ÿåˆ†æä»»åŠ¡
   - ç†è§£è®­ç»ƒå‚æ•°é…ç½®

4. **ç¬¬4å‘¨ï¼šè¿›é˜¶åº”ç”¨**
   - æ¢ç´¢é‡åŒ–æŠ€æœ¯
   - å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒ
   - å°è¯•å¤šæ¨¡æ€ä»»åŠ¡

### å®è·µé¡¹ç›®å»ºè®®

#### åˆçº§é¡¹ç›®

1. **æƒ…æ„Ÿåˆ†æå™¨**
   - ä½¿ç”¨ Pipeline è¿›è¡Œæƒ…æ„Ÿåˆ†æ
   - æ•°æ®é›†ï¼šIMDBã€Rotten Tomatoes
   - ç›®æ ‡ï¼šç†è§£ Pipeline åŸºæœ¬ç”¨æ³•

2. **æ–‡æœ¬åˆ†ç±»**
   - ä½¿ç”¨ Trainer å¾®è°ƒ DistilBERT
   - æ•°æ®é›†ï¼šAG Newsã€DBpedia
   - ç›®æ ‡ï¼šæŒæ¡æ¨¡å‹å¾®è°ƒæµç¨‹

#### ä¸­çº§é¡¹ç›®

3. **é—®ç­”ç³»ç»Ÿ**
   - ä½¿ç”¨ BERT è¿›è¡Œé—®ç­”
   - æ•°æ®é›†ï¼šSQuAD
   - ç›®æ ‡ï¼šç†è§£ä¸Šä¸‹æ–‡ç†è§£ä»»åŠ¡

4. **æ–‡æœ¬æ‘˜è¦**
   - ä½¿ç”¨ T5 æˆ– BART
   - æ•°æ®é›†ï¼šCNN/DailyMail
   - ç›®æ ‡ï¼šå­¦ä¹ åºåˆ—åˆ°åºåˆ—ä»»åŠ¡

#### é«˜çº§é¡¹ç›®

5. **å¯¹è¯ç³»ç»Ÿ**
   - ä½¿ç”¨ GPT ç³»åˆ—æ¨¡å‹
   - æ•°æ®é›†ï¼šè‡ªå®šä¹‰å¯¹è¯æ•°æ®
   - ç›®æ ‡ï¼šæŒæ¡ç”Ÿæˆå¼ä»»åŠ¡

6. **å¤šæ¨¡æ€åº”ç”¨**
   - ä½¿ç”¨ CLIP æˆ– BLIP
   - ä»»åŠ¡ï¼šå›¾åƒæè¿°ã€è§†è§‰é—®ç­”
   - ç›®æ ‡ï¼šæ¢ç´¢å¤šæ¨¡æ€æ¨¡å‹

---

## ğŸ”— å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£

- **ä¸­æ–‡æ–‡æ¡£é¦–é¡µ**: https://hugging-face.cn/docs/transformers/
- **å¿«é€Ÿå…¥é—¨**: https://hugging-face.cn/docs/transformers/quicktour
- **å®‰è£…æŒ‡å—**: https://hugging-face.cn/docs/transformers/installation
- **API å‚è€ƒ**: https://hugging-face.cn/docs/transformers/main_classes/pipelines

### æ¨èè¯¾ç¨‹

- **LLM è¯¾ç¨‹**: https://hugging-face.cn/learn/llm-course/chapter1/1#pt
  - æ¶µç›– Transformer æ¨¡å‹å·¥ä½œåŸç†
  - åŒ…å«ç†è®ºå’Œå®è·µç»ƒä¹ 
  - ä»æ•°æ®é›†æ•´ç†åˆ°æ¨¡å‹å¾®è°ƒçš„å®Œæ•´æµç¨‹

### ç¤¾åŒºèµ„æº

- **Hugging Face Hub**: https://hugging-face.cn/models
  - è¶…è¿‡ 100 ä¸‡ä¸ªé¢„è®­ç»ƒæ¨¡å‹
  - å¯ç›´æ¥ä¸‹è½½å’Œä½¿ç”¨
  - æ”¯æŒæ¨¡å‹åˆ†äº«å’Œåä½œ

- **Hugging Face è®ºå›**: https://discuss.huggingface.co/
  - æŠ€æœ¯é—®é¢˜è®¨è®º
  - æœ€ä½³å®è·µåˆ†äº«
  - ç¤¾åŒºæ”¯æŒ

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. æ¨¡å‹é€‰æ‹©

- **æ ¹æ®ä»»åŠ¡é€‰æ‹©æ¨¡å‹**ï¼šä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„æœ€ä½³æ¨¡å‹
- **è€ƒè™‘èµ„æºé™åˆ¶**ï¼šå¤§æ¨¡å‹éœ€è¦æ›´å¤š GPU å†…å­˜
- **æŸ¥çœ‹æ¨¡å‹å¡ç‰‡**ï¼šäº†è§£æ¨¡å‹çš„è®­ç»ƒæ•°æ®å’Œé™åˆ¶

### 2. æ€§èƒ½ä¼˜åŒ–

- **ä½¿ç”¨ `device_map="auto"`**ï¼šè‡ªåŠ¨åˆ†é…æ¨¡å‹åˆ°æœ€å¿«è®¾å¤‡
- **ä½¿ç”¨ `torch_dtype="auto"`**ï¼šé¿å…é‡å¤åŠ è½½æƒé‡
- **æ‰¹é‡å¤„ç†**ï¼šæé«˜æ¨ç†æ•ˆç‡
- **é‡åŒ–æŠ€æœ¯**ï¼šå‡å°‘å†…å­˜å ç”¨

### 3. æ•°æ®å¤„ç†

- **ä½¿ç”¨ `datasets` åº“**ï¼šé«˜æ•ˆçš„æ•°æ®åŠ è½½å’Œå¤„ç†
- **æ‰¹é‡åˆ†è¯**ï¼šä½¿ç”¨ `batched=True` æé«˜é€Ÿåº¦
- **æ•°æ®ç¼“å­˜**ï¼šé¿å…é‡å¤å¤„ç†

### 4. è®­ç»ƒæŠ€å·§

- **ä»å°æ¨¡å‹å¼€å§‹**ï¼šå…ˆç”¨å°æ¨¡å‹éªŒè¯æµç¨‹
- **ç›‘æ§è®­ç»ƒè¿‡ç¨‹**ï¼šä½¿ç”¨ TensorBoard æˆ– wandb
- **ä¿å­˜æ£€æŸ¥ç‚¹**ï¼šå®šæœŸä¿å­˜æ¨¡å‹é¿å…ä¸¢å¤±
- **è¯„ä¼°éªŒè¯é›†**ï¼šé¿å…è¿‡æ‹Ÿåˆ

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¼€å§‹

1. **å®‰è£…ç¯å¢ƒ**
   ```bash
   pip install torch transformers datasets evaluate accelerate
   ```

2. **è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹**
   ```python
   from transformers import pipeline

   # åˆ›å»ºæƒ…æ„Ÿåˆ†æ pipeline
   classifier = pipeline("sentiment-analysis")

   # æµ‹è¯•
   result = classifier("I love learning Transformers!")
   print(result)
   ```

3. **æµè§ˆ Hugging Face Hub**
   - è®¿é—® https://hugging-face.cn/models
   - æœç´¢æ„Ÿå…´è¶£çš„æ¨¡å‹
   - æŸ¥çœ‹æ¨¡å‹å¡ç‰‡å’Œä½¿ç”¨ç¤ºä¾‹

### æ·±å…¥å­¦ä¹ 

1. **å®Œæˆå¿«é€Ÿå…¥é—¨æ•™ç¨‹**
   - è·Ÿéšå®˜æ–¹æ–‡æ¡£é€æ­¥å­¦ä¹ 
   - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ä»£ç 
   - ç†è§£æ¯ä¸ªæ¦‚å¿µ

2. **é€‰æ‹©ä¸€ä¸ªé¡¹ç›®**
   - ä»åˆçº§é¡¹ç›®å¼€å§‹
   - é€æ­¥å¢åŠ éš¾åº¦
   - è®°å½•å­¦ä¹ ç¬”è®°

3. **å‚ä¸ç¤¾åŒº**
   - åœ¨è®ºå›æé—®å’Œå›ç­”
   - åˆ†äº«ä½ çš„æ¨¡å‹å’Œç»éªŒ
   - å…³æ³¨æœ€æ–°è¿›å±•

---

## ğŸ“ å­¦ä¹ è¿›åº¦è¿½è¸ª

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å…¥é—¨ âœ…/âŒ

- [ ] å®Œæˆç¯å¢ƒé…ç½®
- [ ] åˆ›å»º Hugging Face è´¦æˆ·
- [ ] è¿è¡Œç¬¬ä¸€ä¸ª Pipeline ç¤ºä¾‹
- [ ] ç†è§£ä¸‰ä¸ªæ ¸å¿ƒç±»çš„ä½œç”¨
- [ ] æŒæ¡ AutoClass API

### ç¬¬äºŒé˜¶æ®µï¼šæ·±å…¥å­¦ä¹  âœ…/âŒ

- [ ] å®Œæˆæƒ…æ„Ÿåˆ†æé¡¹ç›®
- [ ] å­¦ä¹  Trainer ä½¿ç”¨
- [ ] ç†è§£è®­ç»ƒå‚æ•°é…ç½®
- [ ] å®Œæˆæ¨¡å‹å¾®è°ƒ
- [ ] å°†æ¨¡å‹æ¨é€åˆ° Hub

### ç¬¬ä¸‰é˜¶æ®µï¼šé«˜çº§åº”ç”¨ âœ…/âŒ

- [ ] æ¢ç´¢é‡åŒ–æŠ€æœ¯
- [ ] å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒ
- [ ] å®Œæˆå¤šæ¨¡æ€é¡¹ç›®
- [ ] ä¼˜åŒ–æ¨ç†æ€§èƒ½
- [ ] éƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ

---

## ğŸ”§ é™„å½•ï¼šå¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ

**ç­”**: æ ¹æ®ä»¥ä¸‹å› ç´ é€‰æ‹©ï¼š
- **ä»»åŠ¡ç±»å‹**ï¼šæ–‡æœ¬åˆ†ç±»ç”¨ BERTï¼Œæ–‡æœ¬ç”Ÿæˆç”¨ GPT
- **èµ„æºé™åˆ¶**ï¼šGPU å†…å­˜æœ‰é™é€‰æ‹©å°æ¨¡å‹ï¼ˆå¦‚ DistilBERTï¼‰
- **æ€§èƒ½è¦æ±‚**ï¼šè¿½æ±‚æœ€ä½³æ€§èƒ½é€‰æ‹©å¤§æ¨¡å‹ï¼ˆå¦‚ Llama-2ï¼‰
- **æ¨ç†é€Ÿåº¦**ï¼šéœ€è¦å¿«é€Ÿæ¨ç†é€‰æ‹©é‡åŒ–æ¨¡å‹

### Q2: æ¨¡å‹åŠ è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**ç­”**: å¸¸è§è§£å†³æ–¹æ¡ˆï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ä½¿ç”¨å›½å†…é•œåƒï¼š`export HF_ENDPOINT=https://hf-mirror.com`
3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶
4. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³

### Q3: GPU å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**ç­”**: ä¼˜åŒ–æ–¹æ¡ˆï¼š
- å‡å°æ‰¹é‡å¤§å°ï¼ˆ`batch_size`ï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆ`gradient_accumulation_steps`ï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ`fp16=True`ï¼‰
- ä½¿ç”¨æ¨¡å‹é‡åŒ–
- ä½¿ç”¨ `device_map="auto"` è‡ªåŠ¨åˆ†é…

### Q4: å¦‚ä½•åŠ é€Ÿè®­ç»ƒï¼Ÿ

**ç­”**: åŠ é€ŸæŠ€å·§ï¼š
- ä½¿ç”¨å¤š GPU è®­ç»ƒ
- å¯ç”¨æ··åˆç²¾åº¦ï¼ˆ`fp16=True`ï¼‰
- ä½¿ç”¨ `torch.compile()`
- ä½¿ç”¨ FlashAttention
- ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆå¢åŠ  `num_workers`ï¼‰

### Q5: å¦‚ä½•ä¿å­˜å’ŒåŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼Ÿ

**ç­”**: ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š
```python
# ä¿å­˜æ¨¡å‹
model.save_pretrained("./my-model")
tokenizer.save_pretrained("./my-model")

# åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("./my-model")
tokenizer = AutoTokenizer.from_pretrained("./my-model")
```

---

## ğŸ“Š ä¸ PyTorch çš„å…³ç³»

Transformers æ˜¯åŸºäº PyTorch æ„å»ºçš„é«˜çº§åº“ï¼Œä¸¤è€…å…³ç³»å¦‚ä¸‹ï¼š

| å±‚çº§ | è¯´æ˜ |
|------|------|
| **PyTorch** | åº•å±‚æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›å¼ é‡æ“ä½œå’Œè‡ªåŠ¨å¾®åˆ† |
| **Transformers** | é«˜çº§æ¨¡å‹åº“ï¼Œæä¾›é¢„è®­ç»ƒæ¨¡å‹å’Œè®­ç»ƒå·¥å…· |

### å­¦ä¹ å»ºè®®

1. **å…ˆå­¦ PyTorch åŸºç¡€**ï¼ˆä½ å·²å®Œæˆ âœ…ï¼‰
   - å¼ é‡æ“ä½œ
   - è‡ªåŠ¨å¾®åˆ†
   - æ¨¡å‹æ„å»º
   - è®­ç»ƒå¾ªç¯

2. **å†å­¦ Transformers**ï¼ˆå½“å‰é˜¶æ®µï¼‰
   - ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
   - Pipeline æ¨ç†
   - Trainer å¾®è°ƒ
   - æ¨¡å‹éƒ¨ç½²

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Transformers æ˜¯ä»€ä¹ˆ**
   - æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹åº“
   - æ”¯æŒå¤šæ¨¡æ€ä»»åŠ¡
   - è·¨æ¡†æ¶å…¼å®¹

2. **ä¸‰ä¸ªæ ¸å¿ƒç±»**
   - PretrainedConfigï¼ˆé…ç½®ï¼‰
   - PreTrainedModelï¼ˆæ¨¡å‹ï¼‰
   - Preprocessorï¼ˆé¢„å¤„ç†å™¨ï¼‰

3. **ä¸¤ä¸ªæ ¸å¿ƒ API**
   - Pipelineï¼ˆæ¨ç†ï¼‰
   - Trainerï¼ˆè®­ç»ƒï¼‰

4. **å­¦ä¹ è·¯å¾„**
   - ç¬¬1å‘¨ï¼šç¯å¢ƒé…ç½® + Pipeline
   - ç¬¬2å‘¨ï¼šæ¨¡å‹åŠ è½½ + æ¨ç†
   - ç¬¬3å‘¨ï¼šTrainer + å¾®è°ƒ
   - ç¬¬4å‘¨ï¼šé«˜çº§åº”ç”¨

### ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å·²ç»æœ‰äº†å®Œæ•´çš„å­¦ä¹ æŒ‡å—ï¼Œå»ºè®®ï¼š

1. **ç«‹å³å¼€å§‹**ï¼šå®‰è£…ç¯å¢ƒå¹¶è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹
2. **å¾ªåºæ¸è¿›**ï¼šæŒ‰ç…§å­¦ä¹ è·¯çº¿é€æ­¥å­¦ä¹ 
3. **åŠ¨æ‰‹å®è·µ**ï¼šå®Œæˆæ¨èçš„å®è·µé¡¹ç›®
4. **è®°å½•ç¬”è®°**ï¼šåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è®°å½•å¿ƒå¾—

---

**ç¥ä½ å­¦ä¹ é¡ºåˆ©ï¼ğŸš€**

