{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b98249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/HF_download\"\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"/root/autodl-tmp/MODELSCOPE_download\"\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c492cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/MODELSCOPE_download/models/modelscope/Llama-2-7b-ms\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "datasets = Dataset.load_from_disk(\"/root/autodl-tmp/code/test-transformers/data/alpaca_data_zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de13224",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacad891",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63843bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(example):\n",
    "  MAX_LENGTH = 1024\n",
    "  instruction = tokenizer(\"\\n\".join([f\"Human: \" + example[\"instruction\"].strip(), example[\"input\"].strip()]).strip() + \"\\n\\nAssistant: \", add_special_tokens=False)\n",
    "  response = tokenizer(example[\"output\"].strip(), add_special_tokens=False)\n",
    "  input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "  attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "  labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "  if len(input_ids) > MAX_LENGTH:\n",
    "    return {}\n",
    "  return {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"labels\": labels\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(process_function, remove_columns=datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"/root/autodl-tmp/MODELSCOPE_download/models/modelscope/Llama-2-7b-ms\",\n",
    "  dtype=torch.bfloat16,\n",
    "  trust_remote_code=True,\n",
    "  low_cpu_mem_usage=True,\n",
    "  device_map=\"auto\",\n",
    "  quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fe2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "  task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04581dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee061ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "  if \"lora_\" in name:\n",
    "    module.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d29b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c25326",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/code/test-transformers/test-kbit/4bit-training/llama-chatbot\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    # 关闭14G显存，开启7G显存\n",
    "    gradient_checkpointing=True,\n",
    "    # adam_epsilon=1e-4, # 如果使用fp16的需要调大精度范围防止溢出\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac13cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets.select(range(6000)),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2571a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Human: {}\\n{}\".format(\"保持健康的三个提示。\", \"\").strip() + \"\\n\\nAssistant: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Human: 保持健康的三个提示。\\n\\nAssistant:  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e09cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"保持健康的三个提示。\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.decode(model.generate(**ipt, max_length=256, do_sample=True, eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"input_ids\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
