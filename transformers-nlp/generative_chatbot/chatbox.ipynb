{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL\"] = \"1\""
   ],
   "id": "fb3fdcb7347172ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\").to(\"cuda\")"
   ],
   "id": "ba9745e528f93513",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\"Moemu/Muice-Dataset\")"
   ],
   "id": "23fad194d1f077e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.chat_template",
   "id": "1a465fa07bb85851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.config.max_position_embeddings",
   "id": "c39bb296a734a682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(datasets[\"train\"][\"conversation\"][0])",
   "id": "12f4f68946c8005c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def process2messages_function(examples):\n",
    "    all_messages = []\n",
    "\n",
    "    for system, conversation in zip(examples[\"system\"], examples[\"conversation\"]):\n",
    "        messages = []\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system\n",
    "        })\n",
    "        for turn in conversation:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": turn[\"human\"]\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": turn[\"assistant\"]\n",
    "            })\n",
    "        all_messages.append(messages)\n",
    "\n",
    "    messages = tokenizer.apply_chat_template(all_messages, tokenize=False, add_generation_prompt=False)\n",
    "    messages = [re.sub(r\"<think>\\s*</think>\", \"\", message) for message in messages]\n",
    "    return {\"messages\": messages}"
   ],
   "id": "ac06a794b7fe6e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def find_assistant_content_including_end(text):\n",
    "    \"\"\"\n",
    "    返回每一段 assistant 内容的 (start_index, end_index)\n",
    "    start_index = <|im_start|>assistant\\n 后第一个字符位置\n",
    "    end_index = 对应 <|im_end|> 的最后一个字符位置（包含 <|im_end|>）\n",
    "    \"\"\"\n",
    "    pattern = r\"<\\|im_start\\|>assistant\\n(.*?<\\|im_end\\|>\\n)\"\n",
    "    spans = []\n",
    "    for match in re.finditer(pattern, text, flags=re.DOTALL):\n",
    "        start = match.start(1)       # 第一个括号组的开始\n",
    "        end = match.end(1) - 1       # 左闭右闭\n",
    "        spans.append((start, end))\n",
    "    return spans\n",
    "\n",
    "def process_messages2ids_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"messages\"],\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    labels = []\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    for batch_idx in range(len(inputs[\"input_ids\"])):\n",
    "        offsets = offset_mapping[batch_idx]\n",
    "        input_ids = inputs[\"input_ids\"][batch_idx]\n",
    "        label = [220] * len(input_ids)\n",
    "        assistant_contents_idxes = find_assistant_content_including_end(examples[\"messages\"][batch_idx])\n",
    "        assistant_contents_i = 0\n",
    "        for idx, offset in enumerate(offsets):\n",
    "            if assistant_contents_idxes[assistant_contents_i][0] <= offset[0] and offset[1] <= assistant_contents_idxes[assistant_contents_i][1] + 1:\n",
    "                label[idx] = input_ids[idx]\n",
    "            if offset[1] >= assistant_contents_idxes[assistant_contents_i][1] + 1:\n",
    "                assistant_contents_i += 1\n",
    "                if assistant_contents_i == len(assistant_contents_idxes):\n",
    "                    break\n",
    "        labels.append(label)\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "\n",
    "    return inputs"
   ],
   "id": "52e9f99307d4907b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_messages_datasets = datasets.map(process2messages_function, batched=True)\n",
    "\n",
    "tokenized_datasets = to_messages_datasets.filter(lambda x: len(x[\"messages\"]) <= 4096).map(process_messages2ids_function, batched=True, remove_columns=to_messages_datasets[\"train\"].column_names)"
   ],
   "id": "cfe80a543a8004fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.decode(tokenized_datasets[\"train\"][\"input_ids\"][0])",
   "id": "b4aef0ad2feaa5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.decode(tokenized_datasets[\"train\"][\"labels\"][0])",
   "id": "3ede6caef7ed55f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2\n",
    ")"
   ],
   "id": "13fc825e53bcb117",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")"
   ],
   "id": "beaf8ec749464f13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "cfcf03e670149d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.save_model()",
   "id": "bb1000056ddeba63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ],
   "id": "35d822d127b46361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.generation_config",
   "id": "3e04a2c80b6218d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ],
   "id": "71613a07eb2e2392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个名为沐雪的可爱AI女孩子\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(messages, do_sample=True, max_length=4096)\n",
    "outputs"
   ],
   "id": "78daf760f586e1a7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
