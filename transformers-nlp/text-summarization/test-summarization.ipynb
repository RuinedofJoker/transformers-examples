{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:26:52.234499Z",
     "start_time": "2026-02-13T07:26:52.231939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL\"] = \"1\"\n",
    "os.environ[\"USE_DISTRIBUTED\"] = \"0\""
   ],
   "id": "a1b1cf70258800d6",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-13T07:10:21.860969Z",
     "start_time": "2026-02-13T07:10:16.603893Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\"supremezxc/nlpcc_2017\", split=\"train\").shuffle(seed=42).select(range(5000)).train_test_split(test_size=0.02)"
   ],
   "id": "6c71178caabdb8c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:10:14.677778Z",
     "start_time": "2026-02-13T07:09:37.918788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/mengzi-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Langboat/mengzi-t5-base\").to(\"cuda\")"
   ],
   "id": "3edbcdd20b600671",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "807d402952d24e8e976c8477860a0810"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2119f7637e674ff1ac867707e2386b46"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:11:09.360879Z",
     "start_time": "2026-02-13T07:11:09.356983Z"
    }
   },
   "cell_type": "code",
   "source": "datasets[\"train\"][0][\"data\"]",
   "id": "1f0e4238e2f6c968",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '昨天凌晨,新乡原阳县韩董庄乡焦庵村的一个私人家具厂发生了火灾,一个年仅4岁的男孩被大火烧死,而孩子的父母,也就是家具厂老板邵某和妻子被严重烧伤,性命垂危,医生说很可能熬不过两天。这场火灾不是意外,而是有人故意放火,放火的不是仇家,而是邵某的亲生儿子、4岁小男孩的亲哥哥!这一切到底是为什么?睡梦中家中突发大火一死两伤(看视频请点我!)4月12日凌晨5点多,40多岁的邵某和妻儿正在睡梦中,突然,家里燃起大火。邵某的家是一个小型的家具厂,存放着大量的木材、海绵等易燃物,火势很快蔓延开来。当地消防迅速赶到,大火很快被扑灭了。邵某和妻子被救了出来,伤势十分严重,立即被送往医院进行救治,更让人遗憾的是,他们年仅4岁的小儿子被救出时已经停止了呼吸。纵火者竟是亲哥哥纵火后淡定上网打游戏村民说,邵某是安阳滑县人,家里一共四口人,邵某还有一个大儿子,17岁,叫邵明明(化名),可是大家怎么都没有找到他,大清早的他去了哪里?邻居说明明喜欢去网吧,得知这一消息,民警赶紧分头去找,果然在网吧找到了正在玩游戏的明明,民警告诉他家里失火了,令民警没想到的是,明明竟淡定的说:“我知道,火是我放的。”纵火只因玩网游被父亲打骂邵明明为什么在自己家里放火?民警询问得知,前天晚上,因为沉迷上网打游戏,明明挨了爸爸的一顿揍,这让他非常恼火,并怀恨在心。于是趁爸爸妈妈和弟弟都在睡觉的时候,在房间里放了一把火,然后去了附近网吧上网。他没想到,这把火烧死了弟弟,并害的爸爸妈妈住进医院,现在还生死未卜。记者从当地警方了解到,明明小学毕业就辍学了,用村民的话说“这孩子有点憨”,平时什么都不干就喜欢上网打游戏,爸妈说了多少次就是不改。目前,邵明明已经被当地警方刑事拘留。都市君有话说:只因挨了打骂就放火烧家,原因也许不会这么简单,但它的确是悲剧产生的导火索。记得曾有教育专家在谈到棍棒教育时忠告说:你的暴力倾向会在孩子身上发扬光大。可不,简单粗暴的做法就换来了极端的报复,这也给做父母的敲了个警钟。',\n",
       " 'title': '组图:新乡17岁男孩沉迷网游被父亲打骂,怀恨纵火不料烧死4岁弟弟,事后淡定去网吧;父母也被严重烧伤生命垂危。'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:16:37.837389Z",
     "start_time": "2026-02-13T07:16:37.834192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_function(examples):\n",
    "    contents = [\"摘要生成: \\n\" + v[\"content\"] for v in examples[\"data\"]]\n",
    "    labels = [v[\"title\"] for v in examples[\"data\"]]\n",
    "\n",
    "    inputs_contents = tokenizer(contents, max_length=384, truncation=True)\n",
    "    inputs_labels = tokenizer(labels, max_length=64, truncation=True)\n",
    "\n",
    "    inputs_contents[\"labels\"] = inputs_labels[\"input_ids\"]\n",
    "\n",
    "    return inputs_contents\n"
   ],
   "id": "e921ef99c150ea38",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:16:40.584709Z",
     "start_time": "2026-02-13T07:16:39.218789Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)",
   "id": "26773bad140939ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4900 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86b1893093784cb385e4edb07daafccf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af092bc6b46f4d0bb8371701b1a4a86d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:23:31.512712Z",
     "start_time": "2026-02-13T07:23:31.509048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rouge_chinese import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def compute_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    decode_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decode_preds = [\" \".join(p) for p in decode_preds]\n",
    "    decode_labels = [\" \".join(l) for l in decode_labels]\n",
    "    scores = rouge.get_scores(decode_preds, decode_labels, avg=True)\n",
    "    print(scores)\n",
    "    return {\n",
    "        \"rouge-1\": scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge-2\": scores[\"rouge-2\"][\"f\"],\n",
    "        \"rouge-l\": scores[\"rouge-l\"][\"f\"],\n",
    "    }"
   ],
   "id": "ddb8e9a895580f55",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:23:32.738106Z",
     "start_time": "2026-02-13T07:23:32.688511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"rouge-l\",\n",
    "    predict_with_generate=True\n",
    ")"
   ],
   "id": "b561adb90568475f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:26:56.570372Z",
     "start_time": "2026-02-13T07:26:56.138976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metric,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    ")"
   ],
   "id": "8867d3a604be50db",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'Seq2SeqTrainer'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001B[39m, in \u001B[36m_LazyModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   2044\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2045\u001B[39m     module = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2046\u001B[39m     value = \u001B[38;5;28mgetattr\u001B[39m(module, name)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2239\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   2238\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m2239\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2237\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   2236\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2237\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   2238\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\importlib\\__init__.py:88\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     87\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1387\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1360\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1331\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:935\u001B[39m, in \u001B[36m_load_unlocked\u001B[39m\u001B[34m(spec)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:1023\u001B[39m, in \u001B[36mexec_module\u001B[39m\u001B[34m(self, module)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:488\u001B[39m, in \u001B[36m_call_with_frames_removed\u001B[39m\u001B[34m(f, *args, **kwds)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:23\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdistributed\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfsdp\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FullyShardedDataParallel\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\torch\\distributed\\fsdp\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_flat_param\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FlatParameter \u001B[38;5;28;01mas\u001B[39;00m FlatParameter\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_fully_shard\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      3\u001B[39m     CPUOffloadPolicy,\n\u001B[32m      4\u001B[39m     FSDPModule,\n\u001B[32m   (...)\u001B[39m\u001B[32m     10\u001B[39m     UnshardHandle,\n\u001B[32m     11\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\torch\\distributed\\fsdp\\_flat_param.py:31\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mparameter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _ParameterMeta  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtesting\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdistributed\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_pg\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeProcessGroup\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_fsdp_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     34\u001B[39m     _ext_post_unflatten_transform,\n\u001B[32m     35\u001B[39m     _ext_pre_flatten_transform,\n\u001B[32m     36\u001B[39m     FSDPExtensions,\n\u001B[32m     37\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\torch\\testing\\_internal\\distributed\\fake_pg.py:4\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdistributed\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdist\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_C\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_distributed_c10d\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeProcessGroup\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mFakeStore\u001B[39;00m(dist.Store):\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Seq2SeqTrainer, DataCollatorForSeq2Seq\n\u001B[32m      2\u001B[39m trainer = Seq2SeqTrainer(\n\u001B[32m      3\u001B[39m     model=model,\n\u001B[32m      4\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer)\n\u001B[32m      9\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2133\u001B[39m, in \u001B[36m_LazyModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   2129\u001B[39m                     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\n\u001B[32m   2130\u001B[39m                         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not import module \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m. Are this object\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms requirements defined correctly?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2131\u001B[39m                     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   2132\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2133\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\n\u001B[32m   2134\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not import module \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m. Are this object\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms requirements defined correctly?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2135\u001B[39m             ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   2137\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._modules:\n\u001B[32m   2138\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: Could not import module 'Seq2SeqTrainer'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:28:19.826552Z",
     "start_time": "2026-02-13T07:28:19.808408Z"
    }
   },
   "cell_type": "code",
   "source": "from torch._C._distributed_c10d import FakeProcessGroup",
   "id": "1d00e9b5cac6f99f",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_C\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_distributed_c10d\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeProcessGroup\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
