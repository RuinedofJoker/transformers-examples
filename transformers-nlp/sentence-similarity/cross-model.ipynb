{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:03.904098Z",
     "start_time": "2026-02-13T01:46:03.900928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL\"] = \"1\""
   ],
   "id": "c4fe5846708be800",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:12.631831Z",
     "start_time": "2026-02-13T01:46:06.414511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ],
   "id": "1a8efeb3238dd081",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:32.046068Z",
     "start_time": "2026-02-13T01:46:30.018385Z"
    }
   },
   "cell_type": "code",
   "source": "datasets = load_dataset(\"json\", data_files=\"./train_pair_1w.json\", split=\"train\").train_test_split(test_size=0.2)",
   "id": "b59bc7869750a9c8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets[\"train\"]",
   "id": "aad9214b087f6e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:34.407277Z",
     "start_time": "2026-02-13T01:46:32.800817Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")",
   "id": "38d30ff586bd988b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:36.365530Z",
     "start_time": "2026-02-13T01:46:36.362425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_function(examples, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = [float(v) for v in examples[\"label\"]]\n",
    "    return inputs"
   ],
   "id": "14d643dbaca6bf8f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:40.780076Z",
     "start_time": "2026-02-13T01:46:39.676647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ],
   "id": "f3efe4a6ecef37f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f7c076ec6804319acbf811efb15fcb2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3601587b1aae4fcf88cedd3c5e1a8387"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_datasets[\"train\"]",
   "id": "70dd313ba68b3c09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:46.193809Z",
     "start_time": "2026-02-13T01:46:44.336421Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=1).to(\"cuda\")",
   "id": "1cf5881d8fac1496",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48002191d01d4324b9432569fae393df"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1mBertForSequenceClassification LOAD REPORT\u001B[0m from: hfl/chinese-macbert-base\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "bert.embeddings.position_ids               | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.bias               | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001B[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001B[0m\n",
      "Exception in thread Thread-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\"\u001B[0m, line \u001B[35m657\u001B[0m, in \u001B[35mhf_raise_for_status\u001B[0m\n",
      "    \u001B[31mresponse.raise_for_status\u001B[0m\u001B[1;31m()\u001B[0m\n",
      "    \u001B[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\httpx\\_models.py\"\u001B[0m, line \u001B[35m829\u001B[0m, in \u001B[35mraise_for_status\u001B[0m\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "\u001B[1;35mhttpx.HTTPStatusError\u001B[0m: \u001B[35mClient error '403 Forbidden' for url 'https://huggingface.co/api/models/hfl/chinese-macbert-base/discussions?p=0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\u001B[0m\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\threading.py\"\u001B[0m, line \u001B[35m1044\u001B[0m, in \u001B[35m_bootstrap_inner\u001B[0m\n",
      "    \u001B[31mself.run\u001B[0m\u001B[1;31m()\u001B[0m\n",
      "    \u001B[31m~~~~~~~~\u001B[0m\u001B[1;31m^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\threading.py\"\u001B[0m, line \u001B[35m995\u001B[0m, in \u001B[35mrun\u001B[0m\n",
      "    \u001B[31mself._target\u001B[0m\u001B[1;31m(*self._args, **self._kwargs)\u001B[0m\n",
      "    \u001B[31m~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\safetensors_conversion.py\"\u001B[0m, line \u001B[35m117\u001B[0m, in \u001B[35mauto_conversion\u001B[0m\n",
      "    raise e\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\safetensors_conversion.py\"\u001B[0m, line \u001B[35m96\u001B[0m, in \u001B[35mauto_conversion\u001B[0m\n",
      "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\safetensors_conversion.py\"\u001B[0m, line \u001B[35m69\u001B[0m, in \u001B[35mget_conversion_pr_reference\u001B[0m\n",
      "    pr = previous_pr(api, model_id, pr_title, token=token)\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\transformers\\safetensors_conversion.py\"\u001B[0m, line \u001B[35m14\u001B[0m, in \u001B[35mprevious_pr\u001B[0m\n",
      "    for discussion in \u001B[31mget_repo_discussions\u001B[0m\u001B[1;31m(repo_id=model_id, token=token)\u001B[0m:\n",
      "                      \u001B[31m~~~~~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\huggingface_hub\\hf_api.py\"\u001B[0m, line \u001B[35m6350\u001B[0m, in \u001B[35mget_repo_discussions\u001B[0m\n",
      "    discussions, has_next = \u001B[31m_fetch_discussion_page\u001B[0m\u001B[1;31m(page_index=page_index)\u001B[0m\n",
      "                            \u001B[31m~~~~~~~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^^^^^^^^^^^^^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\huggingface_hub\\hf_api.py\"\u001B[0m, line \u001B[35m6339\u001B[0m, in \u001B[35m_fetch_discussion_page\u001B[0m\n",
      "    \u001B[31mhf_raise_for_status\u001B[0m\u001B[1;31m(resp)\u001B[0m\n",
      "    \u001B[31m~~~~~~~~~~~~~~~~~~~\u001B[0m\u001B[1;31m^^^^^^\u001B[0m\n",
      "  File \u001B[35m\"C:\\Users\\61640\\miniconda3\\envs\\test-pytorch\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\"\u001B[0m, line \u001B[35m724\u001B[0m, in \u001B[35mhf_raise_for_status\u001B[0m\n",
      "    raise _format(HfHubHTTPError, message, response) from e\n",
      "\u001B[1;35mhuggingface_hub.errors.HfHubHTTPError\u001B[0m: \u001B[35m(Request ID: Root=1-698e8286-4a1ff44b220728eb0c0104aa;89bb31c6-2973-4c4a-93a7-fa111786c400)\n",
      "\n",
      "403 Forbidden: Discussions are disabled for this repo.\n",
      "Cannot access content at: https://huggingface.co/api/models/hfl/chinese-macbert-base/discussions?p=0.\n",
      "Make sure your token has the correct permissions.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:55.124341Z",
     "start_time": "2026-02-13T01:46:49.659040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")"
   ],
   "id": "4da26a43e3e3efb4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:46:59.851210Z",
     "start_time": "2026-02-13T01:46:59.847705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = [int((p > 0.5).item()) for p in predictions]\n",
    "    labels = [int(l) for l in labels]\n",
    "    accuracy_metric = accuracy.compute(predictions=predictions, references=labels)\n",
    "    accuracy_metric.update(f1.compute(predictions=predictions, references=labels))\n",
    "    return accuracy_metric"
   ],
   "id": "679d736f01c6880d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class MyTrainerCallback(TrainerCallback):\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0 and len(state.log_history) > 0:\n",
    "            print(state.log_history[len(state.log_history) - 1])"
   ],
   "id": "170af237ed52cf72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:47:14.316985Z",
     "start_time": "2026-02-13T01:47:14.304272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./cross_model\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    log_level=\"info\",\n",
    "    log_level_replica=\"info\",\n",
    "    logging_first_step=True\n",
    ")"
   ],
   "id": "a4d48d3c42ba418",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:47:16.797608Z",
     "start_time": "2026-02-13T01:47:16.781665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metric,\n",
    "    # callbacks=[MyTrainerCallback()]\n",
    ")"
   ],
   "id": "5e07d1116773de2f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:53:23.633163Z",
     "start_time": "2026-02-13T01:47:23.133807Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "228af466ce6a3b39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8,000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 102,268,417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 05:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.130455</td>\n",
       "      <td>0.084607</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>0.851827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.120786</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.870432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.082128</td>\n",
       "      <td>0.087688</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.844477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.079608</td>\n",
       "      <td>0.068191</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>0.888308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.068918</td>\n",
       "      <td>0.064208</td>\n",
       "      <td>0.918500</td>\n",
       "      <td>0.891694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.056298</td>\n",
       "      <td>0.067615</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.890656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.059931</td>\n",
       "      <td>0.066182</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.891233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "46614a8eb9d423ab05a78a24622d4427"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-100\n",
      "Configuration saved in ./cross_model\\checkpoint-100\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c966d3eb9b8242b4bc72fa742bd574c6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-100\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-100\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-200\n",
      "Configuration saved in ./cross_model\\checkpoint-200\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaf409b9163a49c487c5f8fd5eedfc5a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-200\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-200\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-300\n",
      "Configuration saved in ./cross_model\\checkpoint-300\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "418d4f35f66240a0a58f168bd978e8bc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-300\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-300\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-400\n",
      "Configuration saved in ./cross_model\\checkpoint-400\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35bd256951824d0db7e5e61428db1c73"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-400\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-400\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-500\n",
      "Configuration saved in ./cross_model\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "056786bff96849a0b516b7be88615ca0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-500\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-500\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-600\n",
      "Configuration saved in ./cross_model\\checkpoint-600\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e19553a82c6d4a5abddf2d915d6175e6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-600\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-600\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-700\n",
      "Configuration saved in ./cross_model\\checkpoint-700\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfff4c4f078a431e9c58e135bb276afc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-700\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-700\\tokenizer_config.json\n",
      "Saving model checkpoint to ./cross_model\\checkpoint-750\n",
      "Configuration saved in ./cross_model\\checkpoint-750\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d9de2c941544f228eed963e8be94005"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./cross_model\\checkpoint-750\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in ./cross_model\\checkpoint-750\\tokenizer_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./cross_model\\checkpoint-500 (score: 0.8916943521594685).\n",
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.08427901224295298, metrics={'train_runtime': 360.3861, 'train_samples_per_second': 66.595, 'train_steps_per_second': 2.081, 'total_flos': 3157304315904000.0, 'train_loss': 0.08427901224295298, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:55:55.506433Z",
     "start_time": "2026-02-13T01:55:55.432215Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate(tokenized_datasets[\"test\"].select(range(10)))",
   "id": "672c9114dc49a3c0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "38af64891a40f80f5860790f28d5bab6"
     }
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03167562931776047,\n",
       " 'eval_accuracy': 0.9,\n",
       " 'eval_f1': 0.8571428571428571,\n",
       " 'eval_runtime': 0.0678,\n",
       " 'eval_samples_per_second': 147.541,\n",
       " 'eval_steps_per_second': 14.754,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:55:44.473641Z",
     "start_time": "2026-02-13T01:55:44.471055Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.id2label = {0: \"不相似\", 1: \"相似\"}",
   "id": "1b140b5a1fa49766",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:55:45.200951Z",
     "start_time": "2026-02-13T01:55:45.198021Z"
    }
   },
   "cell_type": "code",
   "source": "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)",
   "id": "8863511dd0026a02",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T01:55:46.876003Z",
     "start_time": "2026-02-13T01:55:46.698933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = pipe({\"text\": \"我喜欢北京\", \"text_pair\": \"天气怎样\"}, function_to_apply=\"none\")\n",
    "result[\"label\"] =\"相似\" if result[\"score\"] > 0.5 else \"不相似\"\n",
    "result"
   ],
   "id": "de229a548d8f4e6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': '不相似', 'score': 0.006054386030882597}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
