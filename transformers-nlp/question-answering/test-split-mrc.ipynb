{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL\"] = \"1\""
   ],
   "id": "ad3b95c31506ad82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, DefaultDataCollator, TrainingArguments, Trainer, \\\n",
    "    pipeline\n",
    "from datasets import load_dataset"
   ],
   "id": "a7b915c16056c2a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets = load_dataset(\"cmrc2018\")\n",
    "datasets"
   ],
   "id": "e84bea353e716196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets[\"train\"][0]",
   "id": "4e1eb1fdd4a66bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")",
   "id": "ba99dddf4cb21f9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# seed = np.random.randint(0, 2**31 - 1)\n",
    "# print(seed)\n",
    "sample_dataset = datasets[\"train\"].select(range(2))\n",
    "sample_dataset.column_names"
   ],
   "id": "ff5d1cc5d6ba412c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sample_dataset[\"question\"]",
   "id": "64b3e2a95c5cafb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized_examples = tokenizer(\n",
    "    text=list(sample_dataset[\"question\"]),\n",
    "    text_pair=list(sample_dataset[\"context\"]),\n",
    "    return_offsets_mapping=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    stride=128,\n",
    "    max_length=328, truncation=\"only_second\", padding=\"max_length\"\n",
    ")"
   ],
   "id": "976d4c44fcc75aa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_examples.keys()",
   "id": "e98dc8f659a9ff85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(tokenized_examples[\"input_ids\"].shape)\n",
    "print(tokenized_examples[\"offset_mapping\"].shape)\n",
    "print(tokenized_examples[\"overflow_to_sample_mapping\"].shape)"
   ],
   "id": "10994d57da94c7bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_examples[\"overflow_to_sample_mapping\"]",
   "id": "c4391c1dcdcf2741",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_examples[\"offset_mapping\"][0]",
   "id": "b76dc85562e11a72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenized_examples.sequence_ids(0))",
   "id": "a313d2c143612f46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(sample_dataset[\"context\"])",
   "id": "2c3a16f14a6333d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(tokenized_examples[\"input_ids\"])",
   "id": "33292eea6f7d5e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test():\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    overflow_to_sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    for batch_idx in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        real_batch_idx = overflow_to_sample_mapping[batch_idx].item()\n",
    "        answers = sample_dataset[\"answers\"][real_batch_idx]\n",
    "        answer_char_start = answers[\"answer_start\"][0]\n",
    "        answer_char_end = answer_char_start + len(answers[\"text\"][0]) - 1\n",
    "\n",
    "        context_ids_start = tokenized_examples.sequence_ids(batch_idx).index(1)\n",
    "        context_ids_end = tokenized_examples.sequence_ids(batch_idx).index(None, context_ids_start) - 1\n",
    "\n",
    "        offset = tokenized_examples[\"offset_mapping\"][batch_idx]\n",
    "\n",
    "        answer_idx_start = None\n",
    "        answer_idx_end = None\n",
    "\n",
    "        print(sample_dataset[\"answers\"][real_batch_idx])\n",
    "        # print(sample_dataset[\"context\"][real_batch_idx])\n",
    "        #\n",
    "        # print(offset[context_ids_start][0])\n",
    "        # print(offset[context_ids_end][1])\n",
    "        # print(tokenizer.decode(tokenized_examples[\"input_ids\"][batch_idx][offset[context_ids_start][0]:offset[context_ids_end][1]]))\n",
    "\n",
    "        if offset[context_ids_start][0] <= answer_char_start and answer_char_end < offset[context_ids_end][1]:\n",
    "            for idx in range(context_ids_start, context_ids_end + 1):\n",
    "                cur_offset = offset[idx]\n",
    "                if answer_idx_start is None and cur_offset[0] <= answer_char_start < cur_offset[1]:\n",
    "                    answer_idx_start = idx\n",
    "                find_end = False\n",
    "                if answer_idx_start is not None and cur_offset[0] <= answer_char_end < cur_offset[1]:\n",
    "                    answer_idx_end = idx\n",
    "                    find_end = True\n",
    "                if answer_idx_start is not None and answer_idx_end is not None and find_end is False:\n",
    "                    break\n",
    "\n",
    "        if answer_idx_start is not None and answer_idx_end is not None:\n",
    "            start_positions.append(answer_idx_start)\n",
    "            end_positions.append(answer_idx_end)\n",
    "\n",
    "            print(tokenizer.decode(tokenized_examples[\"input_ids\"][batch_idx][answer_idx_start:answer_idx_end + 1]))\n",
    "        else:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "\n",
    "    print(start_positions)\n",
    "    print(end_positions)\n",
    "\n",
    "\n",
    "test()"
   ],
   "id": "41cfe052e141dbd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_function(examples, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text=examples[\"question\"],\n",
    "        text_pair=examples[\"context\"],\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=128,\n",
    "        max_length=328, truncation=\"only_second\", padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    overflow_to_sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for batch_idx in range(len(inputs[\"input_ids\"])):\n",
    "        cls_index = inputs[\"input_ids\"][batch_idx].index(tokenizer.cls_token_id)\n",
    "\n",
    "        real_batch_idx = overflow_to_sample_mapping[batch_idx]\n",
    "        answers = examples[\"answers\"][real_batch_idx]\n",
    "        answer_char_start = answers[\"answer_start\"][0]\n",
    "        answer_char_end = answer_char_start + len(answers[\"text\"][0]) - 1\n",
    "\n",
    "        context_ids_start = inputs.sequence_ids(batch_idx).index(1)\n",
    "        context_ids_end = inputs.sequence_ids(batch_idx).index(None, context_ids_start) - 1\n",
    "\n",
    "        offset = offset_mapping[batch_idx]\n",
    "\n",
    "        answer_idx_start = None\n",
    "        answer_idx_end = None\n",
    "\n",
    "        if offset[context_ids_start][0] <= answer_char_start and answer_char_end < offset[context_ids_end][1]:\n",
    "            for idx in range(context_ids_start, context_ids_end + 1):\n",
    "                cur_offset = offset[idx]\n",
    "                if answer_idx_start is None and cur_offset[0] <= answer_char_start < cur_offset[1]:\n",
    "                    answer_idx_start = idx\n",
    "                find_end = False\n",
    "                if answer_idx_start is not None and cur_offset[0] <= answer_char_end < cur_offset[1]:\n",
    "                    answer_idx_end = idx\n",
    "                    find_end = True\n",
    "                if answer_idx_start is not None and answer_idx_end is not None and find_end is False:\n",
    "                    break\n",
    "\n",
    "        if answer_idx_start is not None and answer_idx_end is not None:\n",
    "            start_positions.append(answer_idx_start)\n",
    "            end_positions.append(answer_idx_end)\n",
    "        else:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "\n",
    "        example_ids.append(examples[\"id\"][real_batch_idx])\n",
    "        inputs[\"offset_mapping\"][batch_idx] = [\n",
    "            (v if inputs.sequence_ids(batch_idx)[i] == 1 else None)\n",
    "            for i, v in enumerate(inputs[\"offset_mapping\"][batch_idx])\n",
    "        ]\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"example_ids\"] = example_ids\n",
    "\n",
    "    return inputs"
   ],
   "id": "203f667448d73e11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_datasets = datasets[\"train\"].select(range(10)).map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "test_datasets"
   ],
   "id": "e6d4b2f6b2b215c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)",
   "id": "9aa277b730703c70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets[\"train\"]",
   "id": "73670398c5c8f97b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def metrics(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    print(pred[1])\n",
    "    print(start_logits.shape)\n",
    "    print(end_logits.shape)\n",
    "    return {}"
   ],
   "id": "afcfd6bfc38176cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\").to(\"cuda\")",
   "id": "e1c538346ac316e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoints-for-mrc\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    report_to=[\"tensorboard\"]\n",
    ")"
   ],
   "id": "bdc9ad96c373cecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets[\"train\"].select(range(int(len(processed_datasets[\"train\"]) / 4)))",
   "id": "3eab139af6b130dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets[\"validation\"].select(range(int(len(processed_datasets[\"validation\"]) / 4)))",
   "id": "ef9e29f5486e6a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=processed_datasets[\"train\"].shuffle(seed=42).select(\n",
    "        range(int(len(processed_datasets[\"train\"]) / 10))),\n",
    "    eval_dataset=processed_datasets[\"validation\"].shuffle(seed=42).select(\n",
    "        range(int(len(processed_datasets[\"validation\"]) / 10))),\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metrics\n",
    ")"
   ],
   "id": "6249ebeeb7751522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "1b5cb73cb7c26ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.evaluate(processed_datasets[\"test\"].select(range(1)))",
   "id": "a5db2f99ea9bddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.save_model()",
   "id": "3373362533bf6f6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./checkpoints-for-mrc\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./checkpoints-for-mrc\", local_files_only=True)"
   ],
   "id": "6bf3738c3b80de04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)",
   "id": "fa6b27a6cbd04b23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班。\")",
   "id": "23572e98cdfdb052",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
