{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL\"] = \"1\""
   ],
   "id": "ad3b95c31506ad82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, DefaultDataCollator, TrainingArguments, Trainer, pipeline\n",
    "from datasets import load_dataset"
   ],
   "id": "a7b915c16056c2a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets = load_dataset(\"cmrc2018\")\n",
    "datasets"
   ],
   "id": "e84bea353e716196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets[\"train\"][0]",
   "id": "4e1eb1fdd4a66bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")",
   "id": "ba99dddf4cb21f9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_dataset = datasets[\"train\"].select(range(1))\n",
    "sample_dataset.column_names"
   ],
   "id": "ff5d1cc5d6ba412c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sample_dataset[\"question\"]",
   "id": "64b3e2a95c5cafb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized_examples = tokenizer(\n",
    "    text=list(sample_dataset[\"question\"]),\n",
    "    text_pair=list(sample_dataset[\"context\"]),\n",
    "    return_offsets_mapping=True,\n",
    "    max_length=512, truncation=\"only_second\", padding=\"max_length\"\n",
    ")\n",
    "tokenized_examples.keys()"
   ],
   "id": "976d4c44fcc75aa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenized_examples[\"offset_mapping\"][0])",
   "id": "10994d57da94c7bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenized_examples.sequence_ids(0))",
   "id": "a313d2c143612f46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test():\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for batch_idx, offset in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        answers = sample_dataset[batch_idx][\"answers\"]\n",
    "\n",
    "        answer_start_char = answers[\"answer_start\"][0]\n",
    "        answer_end_char = answer_start_char + len(answers[\"text\"][0]) - 1\n",
    "\n",
    "        # context在input_ids里的起始和结束下标\n",
    "        context_start = tokenized_examples.sequence_ids(batch_idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(batch_idx).index(None, context_start) - 1\n",
    "\n",
    "        answer_start = None\n",
    "        answer_end = None\n",
    "\n",
    "        if answer_start_char >= offset[context_start][0] and answer_end_char <= offset[context_end][1] - 1:\n",
    "            for offset_idx in range(context_start, context_end + 1):\n",
    "                char_start, char_end = offset[offset_idx]\n",
    "                if char_start <= answer_start_char < char_end and answer_start is None:\n",
    "                    answer_start = offset_idx\n",
    "                cur_find_end = False\n",
    "                if answer_start is not None and char_start <= answer_end_char < char_end:\n",
    "                    answer_end = offset_idx\n",
    "                    cur_find_end = True\n",
    "                if answer_start is not None and answer_end is not None and cur_find_end is False:\n",
    "                    break\n",
    "\n",
    "        if answer_start is not None and answer_end is not None:\n",
    "            start_positions.append(answer_start)\n",
    "            end_positions.append(answer_end)\n",
    "        else:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "\n",
    "        print(sample_dataset[batch_idx][\"context\"][offset[answer_start][0]:offset[answer_end][1]])\n",
    "        print(tokenizer.decode(tokenized_examples[\"input_ids\"][batch_idx][answer_start:answer_end + 1]))\n",
    "\n",
    "    print(start_positions)\n",
    "    print(end_positions)"
   ],
   "id": "41cfe052e141dbd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_function(examples, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text=examples[\"question\"],\n",
    "        text_pair=examples[\"context\"],\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=512, truncation=\"only_second\", padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for batch_idx in range(len(inputs[\"input_ids\"])):\n",
    "        offset = offset_mapping[batch_idx]\n",
    "        answers = examples[\"answers\"][batch_idx]\n",
    "\n",
    "        answer_start_char = answers[\"answer_start\"][0]\n",
    "        answer_end_char = answer_start_char + len(answers[\"text\"][0]) - 1\n",
    "\n",
    "        context_start = inputs.sequence_ids(batch_idx).index(1)\n",
    "        context_end = inputs.sequence_ids(batch_idx).index(None, context_start) - 1\n",
    "\n",
    "        answer_start = None\n",
    "        answer_end = None\n",
    "\n",
    "        if answer_start_char >= offset[context_start][0] and answer_end_char <= offset[context_end][1] - 1:\n",
    "            for offset_idx in range(context_start, context_end + 1):\n",
    "                char_start, char_end = offset[offset_idx]\n",
    "                if char_start <= answer_start_char < char_end and answer_start is None:\n",
    "                    answer_start = offset_idx\n",
    "                cur_find_end = False\n",
    "                if answer_start is not None and char_start <= answer_end_char < char_end:\n",
    "                    answer_end = offset_idx\n",
    "                    cur_find_end = True\n",
    "                if answer_start is not None and answer_end is not None and cur_find_end is False:\n",
    "                    break\n",
    "\n",
    "        if answer_start is not None and answer_end is not None:\n",
    "            start_positions.append(answer_start)\n",
    "            end_positions.append(answer_end)\n",
    "        else:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ],
   "id": "203f667448d73e11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)",
   "id": "9aa277b730703c70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets",
   "id": "73670398c5c8f97b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets[\"train\"]",
   "id": "20aea3476124c939",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\").to(\"cuda\")",
   "id": "e1c538346ac316e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoints-for-mrc\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    report_to=[\"tensorboard\"]\n",
    ")"
   ],
   "id": "bdc9ad96c373cecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets[\"train\"].select(range(int(len(processed_datasets[\"train\"]) / 4)))",
   "id": "3eab139af6b130dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_datasets[\"validation\"].select(range(int(len(processed_datasets[\"validation\"]) / 4)))",
   "id": "ef9e29f5486e6a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=processed_datasets[\"train\"].shuffle(seed=42).select(range(int(len(processed_datasets[\"train\"]) / 10))),\n",
    "    eval_dataset=processed_datasets[\"validation\"].shuffle(seed=42).select(range(int(len(processed_datasets[\"validation\"]) / 10))),\n",
    "    data_collator=DefaultDataCollator()\n",
    ")"
   ],
   "id": "6249ebeeb7751522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "1b5cb73cb7c26ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.save_model()",
   "id": "3373362533bf6f6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./checkpoints-for-mrc\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./checkpoints-for-mrc\", local_files_only=True)"
   ],
   "id": "6bf3738c3b80de04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipe= pipeline(\"question-answering\", model=model, tokenizer=tokenizer)",
   "id": "fa6b27a6cbd04b23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班。\")",
   "id": "23572e98cdfdb052",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
